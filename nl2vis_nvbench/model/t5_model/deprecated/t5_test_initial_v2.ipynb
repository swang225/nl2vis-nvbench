{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lab_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext lab_black\n"
     ]
    }
   ],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENVIRONMENT: Local\n",
      "Working directory is: /user/w266/w266-project-carlos\n"
     ]
    }
   ],
   "source": [
    "# Set for local or colab\n",
    "\n",
    "import os\n",
    "from os.path import join\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "ASK_TO_DELETE_LOG_FOLDERS = True\n",
    "\n",
    "\n",
    "def check_create_folder(path: str, ask_to_rm_if_exists=ASK_TO_DELETE_LOG_FOLDERS):\n",
    "    if os.path.exists(path):\n",
    "        if ask_to_rm_if_exists:\n",
    "            response = input(\n",
    "                f\"<{path}>: Already exists.\\n\\nWrite 'del' if you wish to delete other wise press any key\"\n",
    "            )\n",
    "            if response.lower() == \"del\":\n",
    "                print(f\"Deleting: {path}\")\n",
    "                shutil.rmtree(path)\n",
    "\n",
    "                os.makedirs(path)\n",
    "        else:\n",
    "            os.makedirs(path)\n",
    "\n",
    "\n",
    "# Check if running in colab\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "# Project defaults\n",
    "if IN_COLAB:\n",
    "    print(\"ENVIRONMENT: Colab\")\n",
    "\n",
    "    # Mount drive\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount(\"/content/drive\")\n",
    "\n",
    "    # Set the project directory\n",
    "    PROJECT_FOLDER = \"/content/drive/MyDrive/w266/w266-project-carlos\"\n",
    "\n",
    "    # Install dependencies\n",
    "    !pip install -q transformers datasets pytorch-lightning wandb\n",
    "else:\n",
    "    print(\"ENVIRONMENT: Local\")\n",
    "    # Set the project directory\n",
    "    PROJECT_FOLDER = \"/user/w266/w266-project-carlos\"\n",
    "\n",
    "os.chdir(PROJECT_FOLDER)\n",
    "\n",
    "# FOLDERS\n",
    "DATASET_FOLDER = join(PROJECT_FOLDER, \"dataset\")\n",
    "CHECKPOINT_FOLDER = join(PROJECT_FOLDER, \"checkpoints\")\n",
    "MODEL_FOLDER = join(PROJECT_FOLDER, \"saved_models\")\n",
    "LOGGER_FOLDER = join(PROJECT_FOLDER, \"logger\")\n",
    "\n",
    "check_create_folder(CHECKPOINT_FOLDER)\n",
    "check_create_folder(MODEL_FOLDER)\n",
    "check_create_folder(LOGGER_FOLDER)\n",
    "\n",
    "print(f\"Working directory is: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import RobertaTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    T5ForConditionalGeneration,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import WandbLogger, TensorBoardLogger, CSVLogger\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "\n",
    "\n",
    "np.random.seed(123)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load `csv` data as a single `dataframe`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in /user/w266/w266-project-carlos/dataset/train.csv: 12621\n",
      "\n",
      "Number of records in /user/w266/w266-project-carlos/dataset/dev.csv: 717\n",
      "-> Merged!!, 0 duplicates were found and dropped\n",
      "\n",
      "Number of records in /user/w266/w266-project-carlos/dataset/test.csv: 2461\n",
      "-> Merged!!, 0 duplicates were found and dropped\n",
      "\n",
      "A total of 15799 recrods were loaded\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tvBench_id</th>\n",
       "      <th>db_id</th>\n",
       "      <th>chart</th>\n",
       "      <th>hardness</th>\n",
       "      <th>query</th>\n",
       "      <th>question</th>\n",
       "      <th>vega_zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000@y_name@DESC</td>\n",
       "      <td>customers_and_products_contacts</td>\n",
       "      <td>Bar</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Visualize BAR SELECT product_name , COUNT(prod...</td>\n",
       "      <td>Bar chart x axis product name y axis how many ...</td>\n",
       "      <td>mark bar data products encoding x product_name...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2463@x_name@ASC</td>\n",
       "      <td>network_2</td>\n",
       "      <td>Bar</td>\n",
       "      <td>Easy</td>\n",
       "      <td>Visualize BAR SELECT job , min(age) FROM Perso...</td>\n",
       "      <td>how old is the youngest person for each job ? ...</td>\n",
       "      <td>mark bar data person encoding x job y aggregat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2545@y_name@DESC</td>\n",
       "      <td>pets_1</td>\n",
       "      <td>Bar</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Visualize BAR SELECT PetType , avg(pet_age) FR...</td>\n",
       "      <td>Please give me a bar chart to show the average...</td>\n",
       "      <td>mark bar data pets encoding x pettype y aggreg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2615@y_name@ASC</td>\n",
       "      <td>products_for_hire</td>\n",
       "      <td>Bar</td>\n",
       "      <td>Extra Hard</td>\n",
       "      <td>Visualize BAR SELECT payment_date , COUNT(paym...</td>\n",
       "      <td>What are the payment date of the payment with ...</td>\n",
       "      <td>mark bar data payments encoding x payment_date...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1304</td>\n",
       "      <td>election</td>\n",
       "      <td>Bar</td>\n",
       "      <td>Easy</td>\n",
       "      <td>Visualize BAR SELECT County_name , Population ...</td>\n",
       "      <td>What are the name and population of each count...</td>\n",
       "      <td>mark bar data county encoding x county_name y ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         tvBench_id                            db_id chart    hardness  \\\n",
       "0  1000@y_name@DESC  customers_and_products_contacts   Bar      Medium   \n",
       "1   2463@x_name@ASC                        network_2   Bar        Easy   \n",
       "2  2545@y_name@DESC                           pets_1   Bar      Medium   \n",
       "3   2615@y_name@ASC                products_for_hire   Bar  Extra Hard   \n",
       "4              1304                         election   Bar        Easy   \n",
       "\n",
       "                                               query  \\\n",
       "0  Visualize BAR SELECT product_name , COUNT(prod...   \n",
       "1  Visualize BAR SELECT job , min(age) FROM Perso...   \n",
       "2  Visualize BAR SELECT PetType , avg(pet_age) FR...   \n",
       "3  Visualize BAR SELECT payment_date , COUNT(paym...   \n",
       "4  Visualize BAR SELECT County_name , Population ...   \n",
       "\n",
       "                                            question  \\\n",
       "0  Bar chart x axis product name y axis how many ...   \n",
       "1  how old is the youngest person for each job ? ...   \n",
       "2  Please give me a bar chart to show the average...   \n",
       "3  What are the payment date of the payment with ...   \n",
       "4  What are the name and population of each count...   \n",
       "\n",
       "                                           vega_zero  \n",
       "0  mark bar data products encoding x product_name...  \n",
       "1  mark bar data person encoding x job y aggregat...  \n",
       "2  mark bar data pets encoding x pettype y aggreg...  \n",
       "3  mark bar data payments encoding x payment_date...  \n",
       "4  mark bar data county encoding x county_name y ...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def combine_csv_files(csv_paths: list[str], shuffle=False):\n",
    "    \"\"\"\n",
    "    Combine csv data into a single dataframe and checks for duplicate records.\n",
    "\n",
    "    \"\"\"\n",
    "    for i, path in enumerate(csv_paths):\n",
    "        df = pd.read_csv(path)\n",
    "        columns = df.columns\n",
    "\n",
    "        print(f\"Number of records in {path}: {df.shape[0]}\")\n",
    "\n",
    "        if i == 0:\n",
    "            df_full = df\n",
    "            columns_base = columns\n",
    "        else:\n",
    "            if not np.array_equal(columns, columns_base):\n",
    "                raise (Exception(\"Columns do not match\"))\n",
    "\n",
    "            total_records = df_full.shape[0] + df.shape[0]\n",
    "\n",
    "            df_full = (\n",
    "                pd.concat([df_full, df]).drop_duplicates(columns).reset_index(drop=True)\n",
    "            )\n",
    "\n",
    "            records_dropped = total_records - df_full.shape[0]\n",
    "\n",
    "            print(f\"-> Merged!!, {records_dropped} duplicates were found and dropped\")\n",
    "\n",
    "        print(\"\")\n",
    "\n",
    "    if shuffle:\n",
    "        shuffled_indices = np.random.permutation(np.arange(df_full.shape[0]))\n",
    "        df_full = df_full.iloc[shuffled_indices, :].reset_index(drop=True)\n",
    "\n",
    "    print(f\"A total of {df_full.shape[0]} recrods were loaded\")\n",
    "    return df_full\n",
    "\n",
    "\n",
    "df_dataset = combine_csv_files(\n",
    "    [\n",
    "        join(DATASET_FOLDER, \"train.csv\"),\n",
    "        join(DATASET_FOLDER, \"dev.csv\"),\n",
    "        join(DATASET_FOLDER, \"test.csv\"),\n",
    "    ],\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "df_dataset.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 0 > 0:6, 7\n",
      "Group 1 > 7:7, 1\n",
      "Group 2 > 8:8, 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tvBench_id', 'db_id', 'chart', 'hardness', 'query', 'question', 'vega_zero', '__index_level_0__'],\n",
       "    num_rows: 7\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tvBench_id', 'db_id', 'chart', 'hardness', 'query', 'question', 'vega_zero', '__index_level_0__'],\n",
       "    num_rows: 1\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tvBench_id', 'db_id', 'chart', 'hardness', 'query', 'question', 'vega_zero', '__index_level_0__'],\n",
       "    num_rows: 2\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'tvBench_id': '1000@y_name@DESC',\n",
       " 'db_id': 'customers_and_products_contacts',\n",
       " 'chart': 'Bar',\n",
       " 'hardness': 'Medium',\n",
       " 'query': 'Visualize BAR SELECT product_name , COUNT(product_name) FROM products GROUP BY product_name ORDER BY COUNT(product_name) DESC',\n",
       " 'question': 'Bar chart x axis product name y axis how many product name , rank by the Y-axis in desc .',\n",
       " 'vega_zero': 'mark bar data products encoding x product_name y aggregate count product_name transform group x sort y desc',\n",
       " '__index_level_0__': 0}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TOTAL_DATASET_SIZE = df_dataset.shape[0]\n",
    "TOTAL_DATASET_SIZE = 10\n",
    "\n",
    "\n",
    "def get_indexes(total_indices, splits=[0.70, 0.10, 0.20]):\n",
    "    start_index = 0\n",
    "    indices = []\n",
    "    groups = len(splits)\n",
    "\n",
    "    for i, split in enumerate(splits):\n",
    "        count = int(total_indices * split)\n",
    "        end_index = start_index + count\n",
    "\n",
    "        print(f\"Group {i} > {start_index}:{end_index-1}, {count}\")\n",
    "\n",
    "        if i + 1 < groups:\n",
    "            indices.append(np.arange(start_index, end_index))\n",
    "        else:\n",
    "            indices.append(np.arange(start_index, total_indices))\n",
    "\n",
    "        start_index = end_index\n",
    "\n",
    "    return indices\n",
    "\n",
    "\n",
    "# indces = get_indexes(df_dataset.shape[0], splits=[0.75, 0.10, 0.15])\n",
    "indces = get_indexes(TOTAL_DATASET_SIZE, splits=[0.75, 0.10, 0.15])\n",
    "\n",
    "train_datset = Dataset.from_pandas(df_dataset.iloc[indces[0]], split=\"train\")\n",
    "valid_dataset = Dataset.from_pandas(df_dataset.iloc[indces[1]], split=\"validation\")\n",
    "test_dataset = Dataset.from_pandas(df_dataset.iloc[indces[2]], split=\"train\")\n",
    "\n",
    "display(train_datset)\n",
    "display(valid_dataset)\n",
    "display(test_dataset)\n",
    "\n",
    "display(train_datset[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process and tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"Salesforce/codet5-small\")\n",
    "\n",
    "prefix = \"Generate vega_zero code: \"\n",
    "max_input_length = 100\n",
    "max_target_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Dataset({\n",
      "    features: ['tvBench_id', 'db_id', 'chart', 'hardness', 'query', 'question', 'vega_zero', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 7\n",
      "})\n",
      "****************************************************************************************************\n",
      "Validation\n",
      "Dataset({\n",
      "    features: ['tvBench_id', 'db_id', 'chart', 'hardness', 'query', 'question', 'vega_zero', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "****************************************************************************************************\n",
      "Test\n",
      "Dataset({\n",
      "    features: ['tvBench_id', 'db_id', 'chart', 'hardness', 'query', 'question', 'vega_zero', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 2\n",
      "})\n",
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "def preprocess_examples(examples, add_db_info=False):\n",
    "    \"\"\"\n",
    "    This function process the input and targets (labels)\n",
    "\n",
    "    Inputs:\n",
    "    - Adds a prefix question (for t5)\n",
    "    - Tokenizes the input\n",
    "\n",
    "    Targets (labels):\n",
    "    - Tokenizes\n",
    "    - Replaces the padding token index from 0 to -100\n",
    "    \"\"\"\n",
    "    questions = examples[\"question\"]  # inputs\n",
    "    queries = examples[\"query\"]  # targets\n",
    "\n",
    "    inputs = [prefix + question for question in questions]\n",
    "\n",
    "    if add_db_info:\n",
    "        pass\n",
    "\n",
    "    # Tokenize the inputs\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, max_length=max_input_length, padding=\"max_length\", truncation=True\n",
    "    )\n",
    "\n",
    "    # Tokenize the targets\n",
    "    labels = tokenizer(\n",
    "        queries, max_length=max_target_length, padding=\"max_length\", truncation=True\n",
    "    ).input_ids\n",
    "\n",
    "    # important: we need to replace the index of the padding tokens by -100\n",
    "    # such that they are not taken into account by the CrossEntropyLoss\n",
    "    labels_with_ignore_index = []\n",
    "    for labels_example in labels:\n",
    "        labels_example = [label if label != 0 else -100 for label in labels_example]\n",
    "        labels_with_ignore_index.append(labels_example)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels_with_ignore_index\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "# Map the function to each dataset\n",
    "train_datset = train_datset.map(preprocess_examples, batched=True)\n",
    "valid_dataset = valid_dataset.map(preprocess_examples, batched=True)\n",
    "test_dataset = test_dataset.map(preprocess_examples, batched=True)\n",
    "\n",
    "columns = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "\n",
    "# This sets `__getitem__` return format (type and columns). The data formatting is applied on-the-fly.\n",
    "# `__getitem__` is what pulls the batches during training\n",
    "train_datset.set_format(type=\"torch\", columns=columns)\n",
    "valid_dataset.set_format(type=\"torch\", columns=columns)\n",
    "test_dataset.set_format(type=\"torch\", columns=columns)\n",
    "\n",
    "print(\"Training\")\n",
    "print(train_datset)\n",
    "print(\"*\" * 100)\n",
    "\n",
    "print(\"Validation\")\n",
    "print(valid_dataset)\n",
    "print(\"*\" * 100)\n",
    "\n",
    "print(\"Test\")\n",
    "print(test_dataset)\n",
    "\n",
    "# Without the `.set_format`, this would get you all the columns\n",
    "print(train_datset[0].keys())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that the previous is working as intended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The keys for each batch are:\n",
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "****************************************************************************************************\n",
      "Input token ids:\n",
      "tensor([    1,  4625,   331, 11061,    67,  7124,   981,    30,  3756,   635,\n",
      "          326,   563,   471,  1056,  2182,   635,   279,  4653,  4980,   269,\n",
      "         3377,  1846,   666,   635,   326,   619,    17,  4890,   316, 17044,\n",
      "          692,     2,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "****************************************************************************************************\n",
      "Decoded input tokens:\n",
      "<s>Generate vega_zero code: Group by the result and count them by a bar chart, could you list by the x-axis in descending?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "****************************************************************************************************\n",
      "Label token ids:\n",
      "tensor([    1, 25780,   554,   605,   985,  9111,  3438,   269, 12666,    12,\n",
      "         1253,    13,  4571, 21656, 13839,  6953,  3438, 10205,  6953,  3438,\n",
      "        14326,     2,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100])\n",
      "****************************************************************************************************\n",
      "Decoded label tokens:\n",
      "<s>Visualize BAR SELECT Result, COUNT(Result) FROM acceptance GROUP BY Result ORDER BY Result DESC</s>\n"
     ]
    }
   ],
   "source": [
    "sample_dataloader = DataLoader(valid_dataset, batch_size=4)\n",
    "\n",
    "batch = next(iter(sample_dataloader))\n",
    "\n",
    "print(f\"The keys for each batch are:\")\n",
    "print(batch.keys())\n",
    "print(\"*\" * 100)\n",
    "\n",
    "print(\"Input token ids:\")\n",
    "print(batch[\"input_ids\"][0])\n",
    "print(\"*\" * 100)\n",
    "\n",
    "print(\"Decoded input tokens:\")\n",
    "print(tokenizer.decode(batch[\"input_ids\"][0]))\n",
    "print(\"*\" * 100)\n",
    "\n",
    "print(\"Label token ids:\")\n",
    "labels = batch[\"labels\"][0]\n",
    "print(labels)\n",
    "print(\"*\" * 100)\n",
    "\n",
    "print(\"Decoded label tokens:\")\n",
    "print(tokenizer.decode([label for label in labels if label != -100]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare and tune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "train_batch_size = 8\n",
    "test_batch_size = 4\n",
    "\n",
    "train_dataloader = DataLoader(train_datset, shuffle=True, batch_size=train_batch_size)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=test_batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=test_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/usr/local/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "Missing logger folder: /user/w266/w266-project-carlos/logger/My_Logger\n",
      "/usr/local/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 60.5 M\n",
      "-----------------------------------------------------\n",
      "60.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "60.5 M    Total params\n",
      "241.969   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/usr/local/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 147:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, validation_loss=3.060, training_loss=2.160]         "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:54: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "class CodeT5(pl.LightningModule):\n",
    "    def __init__(self, lr=5e-5, num_train_epochs=5, warmup_steps=1000):\n",
    "        super().__init__()\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(\n",
    "            \"Salesforce/codet5-small\"\n",
    "        )\n",
    "        self.save_hyperparameters()\n",
    "        self.training_step_count = 0\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids, attention_mask=attention_mask, labels=labels\n",
    "        )\n",
    "        return outputs\n",
    "\n",
    "    def common_step(self, batch, batch_idx):\n",
    "        # `batch` is a dictionary, the '**' before batch\n",
    "        # allows the 'forward step' to directly unpack the dictionary\n",
    "\n",
    "        outputs = self(**batch)\n",
    "        loss = outputs.loss  # The pretrained model aut calcs the loss\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        self.training_step_count += 1  # for debugging\n",
    "\n",
    "        loss = self.common_step(batch, batch_idx)\n",
    "        self.log(\n",
    "            \"training_loss\",\n",
    "            loss,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self.common_step(batch, batch_idx)\n",
    "        self.log(\n",
    "            \"validation_loss\",\n",
    "            loss,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss = self.common_step(batch, batch_idx)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # create optimizer\n",
    "        optimizer = AdamW(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "        # create learning rate scheduler\n",
    "        # len(train_dataloader) is just the total number of batches\n",
    "\n",
    "        num_train_optimization_steps = self.hparams.num_train_epochs * len(\n",
    "            train_dataloader\n",
    "        )\n",
    "        lr_scheduler = {\n",
    "            \"scheduler\": get_linear_schedule_with_warmup(\n",
    "                optimizer,\n",
    "                num_warmup_steps=self.hparams.warmup_steps,\n",
    "                num_training_steps=num_train_optimization_steps,\n",
    "            ),\n",
    "            \"name\": \"learning_rate\",\n",
    "            \"interval\": \"step\",\n",
    "            \"frequency\": 1,\n",
    "        }\n",
    "\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler}\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return train_dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return valid_dataloader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return test_dataloader\n",
    "\n",
    "\n",
    "model = CodeT5()\n",
    "\n",
    "logger = CSVLogger(save_dir=LOGGER_FOLDER, name=\"My_Logger\")\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"validation_loss\", patience=3, strict=False, verbose=False, mode=\"min\"\n",
    ")\n",
    "lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    default_root_dir=CHECKPOINT_FOLDER,\n",
    "    callbacks=[early_stop_callback, lr_monitor],\n",
    "    logger=logger,\n",
    ")\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.save_pretrained(MODEL_FOLDER)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
