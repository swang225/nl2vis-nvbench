{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext lab_black\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set for local or colab\n",
    "\n",
    "import os\n",
    "from os.path import join\n",
    "import sys\n",
    "\n",
    "# Check if running in colab\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "# Project defaults\n",
    "if IN_COLAB:\n",
    "    print(\"ENVIRONMENT: Colab\")\n",
    "\n",
    "    # Mount drive\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount(\"/content/drive\")\n",
    "\n",
    "    # Set the project directory\n",
    "    PROJECT_FOLDER = \"/content/drive/MyDrive/MIDS/w266/w266-project-carlos\"\n",
    "\n",
    "    # Install dependencies\n",
    "    !pip install -q transformers datasets pytorch-lightning SentencePiece #wandb\n",
    "else:\n",
    "    print(\"ENVIRONMENT: Local\")\n",
    "    # Set the project directory\n",
    "    PROJECT_FOLDER = \"/user/w266/w266-project-carlos\"\n",
    "\n",
    "os.chdir(PROJECT_FOLDER)\n",
    "\n",
    "# FOLDERS\n",
    "DATASET_FOLDER = join(PROJECT_FOLDER, \"dataset/dataset_final\")\n",
    "\n",
    "print(f\"Working directory is: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import RobertaTokenizer, T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "from t5_model_support_functions import load_csv_files, token_to_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set experiment folder and architectbase model type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_FOLDER = join(PROJECT_FOLDER, \"experiments/exp_01_t5-base/\")\n",
    "\n",
    "MODEL_TYPE = \"t5-base\"\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(EXPERIMENT_FOLDER)\n",
    "\n",
    "if \"codet5\" in MODEL_TYPE:\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(join(EXPERIMENT_FOLDER, \"tokenizer\"))\n",
    "else:\n",
    "    tokenizer = T5Tokenizer.from_pretrained(join(EXPERIMENT_FOLDER, \"tokenizer\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load `csv` data as `dataframes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_FEATURES = [\"source\", \"labels\", \"token_types\"]\n",
    "\n",
    "df_train, df_val, df_test = load_csv_files(\n",
    "    [\n",
    "        join(DATASET_FOLDER, \"train.csv\"),\n",
    "        join(DATASET_FOLDER, \"dev.csv\"),\n",
    "        join(DATASET_FOLDER, \"test.csv\"),\n",
    "    ],\n",
    "    focus_columns=TARGET_FEATURES,\n",
    "    drop_duplicates=True,\n",
    "    dropna=True,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"Generate vega_zero code: \"\n",
    "max_input_length = 162\n",
    "max_target_length = 60\n",
    "batch_size = 2\n",
    "\n",
    "DEV_TESTING = True\n",
    "DEV_LENGTH = 4\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Calculated\n",
    "total_batches = int(np.ceil(DEV_LENGTH / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEV_TESTING:\n",
    "    train_dataset = Dataset.from_pandas(df_train.head(DEV_LENGTH), split=\"train\")\n",
    "    val_dataset = Dataset.from_pandas(df_val.head(DEV_LENGTH), split=\"validation\")\n",
    "    test_dataset = Dataset.from_pandas(df_test.head(DEV_LENGTH), split=\"test\")\n",
    "else:\n",
    "    train_dataset = Dataset.from_pandas(df_train, split=\"train\")\n",
    "    val_dataset = Dataset.from_pandas(df_val, split=\"validation\")\n",
    "    test_dataset = Dataset.from_pandas(df_test, split=\"test\")\n",
    "\n",
    "\n",
    "print(train_dataset)\n",
    "print(val_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_examples(examples):\n",
    "    \"\"\"\n",
    "    This function process the input and targets (labels)\n",
    "\n",
    "    Inputs:\n",
    "    - Adds a prefix to the source (for t5)\n",
    "    - Tokenizes the input\n",
    "\n",
    "    Targets (labels):\n",
    "    - Tokenizes\n",
    "    - Replaces the padding token index from 0 to -100\n",
    "    \"\"\"\n",
    "    sources = examples[\"source\"]  # inputs\n",
    "    label_queries = examples[\"labels\"]  # targets\n",
    "\n",
    "    inputs = [prefix + source for source in sources]\n",
    "\n",
    "    # Tokenize the inputs\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=max_input_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(device)\n",
    "\n",
    "    # Tokenize the targets\n",
    "    labels = (\n",
    "        tokenizer(\n",
    "            label_queries,\n",
    "            max_length=max_target_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        .to(device)\n",
    "        .input_ids\n",
    "    )\n",
    "\n",
    "    # important: we need to replace the index of the padding tokens by -100\n",
    "    # such that they are not taken into account by the CrossEntropyLoss\n",
    "    labels_with_ignore_index = []\n",
    "    for label_set in labels:\n",
    "        label_set = [label if label != 0 else -100 for label in label_set]\n",
    "        labels_with_ignore_index.append(label_set)\n",
    "\n",
    "    model_inputs[\"label_tokens\"] = labels_with_ignore_index\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "# Map the function to each dataset\n",
    "train_dataset = train_dataset.map(preprocess_examples, batched=True)\n",
    "val_dataset = val_dataset.map(preprocess_examples, batched=True)\n",
    "test_dataset = test_dataset.map(preprocess_examples, batched=True)\n",
    "\n",
    "columns = [\"source\", \"input_ids\", \"labels\"]\n",
    "\n",
    "# This sets `__getitem__` return format (type and columns). The data formatting is applied on-the-fly.\n",
    "# `__getitem__` is what pulls the batches during training\n",
    "train_dataset.set_format(type=\"torch\", columns=columns)\n",
    "val_dataset.set_format(type=\"torch\", columns=columns)\n",
    "test_dataset.set_format(type=\"torch\", columns=columns)\n",
    "\n",
    "print(\"Training\")\n",
    "print(train_dataset)\n",
    "print(\"*\" * 100)\n",
    "\n",
    "print(\"Validation\")\n",
    "print(val_dataset)\n",
    "print(\"*\" * 100)\n",
    "\n",
    "print(\"Test\")\n",
    "print(test_dataset)\n",
    "\n",
    "# Without the `.set_format`, this would get you all the columns\n",
    "print(train_dataset[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "sources = []\n",
    "predictions = []\n",
    "labels = []\n",
    "\n",
    "for i, batch in enumerate(test_dataloader):\n",
    "    print(f\"Processing batch {i+1} of {total_batches}...\", end=\"\")\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            batch[\"input_ids\"],\n",
    "            num_beams=3,\n",
    "            min_length=15,\n",
    "            max_length=max_target_length,\n",
    "        )\n",
    "\n",
    "        predictions.extend(\n",
    "            tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        )\n",
    "\n",
    "        sources.extend(batch[\"source\"])\n",
    "        labels.extend(batch[\"labels\"])\n",
    "        print(\"COMPLETE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame()\n",
    "df_results[\"source\"] = sources\n",
    "df_results[\"labels\"] = labels\n",
    "df_results[\"prediction\"] = predictions\n",
    "\n",
    "df_results.to_csv(join(EXPERIMENT_FOLDER, \"results.csv\"))\n",
    "\n",
    "df_results.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
