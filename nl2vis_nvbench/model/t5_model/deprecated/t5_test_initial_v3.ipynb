{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set for local or colab\n",
    "\n",
    "import os\n",
    "from os.path import join\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "ASK_TO_DELETE_LOG_FOLDERS = True\n",
    "\n",
    "\n",
    "def check_create_folder(path: str, ask_to_rm_if_exists=ASK_TO_DELETE_LOG_FOLDERS):\n",
    "    if os.path.exists(path):\n",
    "        if ask_to_rm_if_exists:\n",
    "            response = input(\n",
    "                f\"<{path}>: Already exists.\\n\\nWrite 'del' if you wish to delete other wise press any key\"\n",
    "            )\n",
    "            if response.lower() == \"del\":\n",
    "                print(f\"Deleting: {path}\")\n",
    "                shutil.rmtree(path)\n",
    "\n",
    "                os.makedirs(path)\n",
    "    else:\n",
    "        os.makedirs(path)\n",
    "\n",
    "\n",
    "# Check if running in colab\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "# Project defaults\n",
    "if IN_COLAB:\n",
    "    print(\"ENVIRONMENT: Colab\")\n",
    "\n",
    "    # Mount drive\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount(\"/content/drive\")\n",
    "\n",
    "    # Set the project directory\n",
    "    PROJECT_FOLDER = \"/content/drive/MyDrive/w266/w266-project-carlos\"\n",
    "\n",
    "    # Install dependencies\n",
    "    !pip install -q transformers datasets pytorch-lightning wandb\n",
    "else:\n",
    "    print(\"ENVIRONMENT: Local\")\n",
    "    # Set the project directory\n",
    "    PROJECT_FOLDER = \"/user/w266/w266-project-carlos\"\n",
    "\n",
    "os.chdir(PROJECT_FOLDER)\n",
    "\n",
    "# FOLDERS\n",
    "DATASET_FOLDER = join(PROJECT_FOLDER, \"dataset\")\n",
    "CHECKPOINT_FOLDER = join(PROJECT_FOLDER, \"checkpoints\")\n",
    "MODEL_FOLDER = join(PROJECT_FOLDER, \"saved_models\")\n",
    "LOGGER_FOLDER = join(PROJECT_FOLDER, \"logger\")\n",
    "\n",
    "check_create_folder(CHECKPOINT_FOLDER)\n",
    "check_create_folder(MODEL_FOLDER)\n",
    "check_create_folder(LOGGER_FOLDER)\n",
    "\n",
    "print(f\"Working directory is: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "from datasets import Dataset\n",
    "from transformers import RobertaTokenizer\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    T5ForConditionalGeneration,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning.callbacks import (\n",
    "    EarlyStopping,\n",
    "    LearningRateMonitor,\n",
    "    TQDMProgressBar,\n",
    "    Callback,\n",
    ")\n",
    "\n",
    "import math\n",
    "\n",
    "np.random.seed(123)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load `csv` data as a single `dataframe`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_csv_files(csv_paths: list[str], shuffle=False):\n",
    "    \"\"\"\n",
    "    Combine csv data into a single dataframe and checks for duplicate records.\n",
    "\n",
    "    \"\"\"\n",
    "    for i, path in enumerate(csv_paths):\n",
    "        df = pd.read_csv(path)\n",
    "        columns = df.columns\n",
    "\n",
    "        print(f\"Number of records in {path}: {df.shape[0]}\")\n",
    "\n",
    "        if i == 0:\n",
    "            df_full = df\n",
    "            columns_base = columns\n",
    "        else:\n",
    "            if not np.array_equal(columns, columns_base):\n",
    "                raise (Exception(\"Columns do not match\"))\n",
    "\n",
    "            total_records = df_full.shape[0] + df.shape[0]\n",
    "\n",
    "            df_full = (\n",
    "                pd.concat([df_full, df]).drop_duplicates(columns).reset_index(drop=True)\n",
    "            )\n",
    "\n",
    "            records_dropped = total_records - df_full.shape[0]\n",
    "\n",
    "            print(f\"-> Merged!!, {records_dropped} duplicates were found and dropped\")\n",
    "\n",
    "        print(\"\")\n",
    "\n",
    "    if shuffle:\n",
    "        shuffled_indices = np.random.permutation(np.arange(df_full.shape[0]))\n",
    "        df_full = df_full.iloc[shuffled_indices, :].reset_index(drop=True)\n",
    "\n",
    "    print(f\"A total of {df_full.shape[0]} recrods were loaded\")\n",
    "    return df_full\n",
    "\n",
    "\n",
    "df_dataset = combine_csv_files(\n",
    "    [\n",
    "        join(DATASET_FOLDER, \"train.csv\"),\n",
    "        join(DATASET_FOLDER, \"dev.csv\"),\n",
    "        join(DATASET_FOLDER, \"test.csv\"),\n",
    "    ],\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "df_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total records {df_dataset.shape[0]}\")\n",
    "print(\"Records with empyt rows:\")\n",
    "display(df_dataset[df_dataset.isna().values])\n",
    "\n",
    "\n",
    "print(\"Dropping NaN...\")\n",
    "df_dataset.dropna(inplace=True)\n",
    "print(f\"New total records {df_dataset.shape[0]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOTAL_DATASET_SIZE = df_dataset.shape[0]\n",
    "TOTAL_DATASET_SIZE = 10\n",
    "\n",
    "\n",
    "def get_indexes(total_indices, splits=[0.70, 0.10, 0.20]):\n",
    "    start_index = 0\n",
    "    indices = []\n",
    "    groups = len(splits)\n",
    "\n",
    "    for i, split in enumerate(splits):\n",
    "        count = int(total_indices * split)\n",
    "        end_index = start_index + count\n",
    "\n",
    "        print(f\"Group {i} > {start_index}:{end_index-1}, {count}\")\n",
    "\n",
    "        if i + 1 < groups:\n",
    "            indices.append(np.arange(start_index, end_index))\n",
    "        else:\n",
    "            indices.append(np.arange(start_index, total_indices))\n",
    "\n",
    "        start_index = end_index\n",
    "\n",
    "    return indices\n",
    "\n",
    "\n",
    "indces = get_indexes(TOTAL_DATASET_SIZE, splits=[0.60, 0.20, 0.20])\n",
    "\n",
    "train_dataset = Dataset.from_pandas(df_dataset.iloc[indces[0]], split=\"train\")\n",
    "val_dataset = Dataset.from_pandas(df_dataset.iloc[indces[1]], split=\"validation\")\n",
    "test_dataset = Dataset.from_pandas(df_dataset.iloc[indces[2]], split=\"train\")\n",
    "\n",
    "display(train_dataset)\n",
    "display(val_dataset)\n",
    "display(test_dataset)\n",
    "\n",
    "display(train_dataset[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process and tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"Salesforce/codet5-small\")\n",
    "\n",
    "prefix = \"Generate vega_zero code: \"\n",
    "max_input_length = 100\n",
    "max_target_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_examples(examples, add_db_info=False):\n",
    "    \"\"\"\n",
    "    This function process the input and targets (labels)\n",
    "\n",
    "    Inputs:\n",
    "    - Adds a prefix question (for t5)\n",
    "    - Tokenizes the input\n",
    "\n",
    "    Targets (labels):\n",
    "    - Tokenizes\n",
    "    - Replaces the padding token index from 0 to -100\n",
    "    \"\"\"\n",
    "    questions = examples[\"question\"]  # inputs\n",
    "    queries = examples[\"query\"]  # targets\n",
    "\n",
    "    inputs = [prefix + question for question in questions]\n",
    "\n",
    "    if add_db_info:\n",
    "        pass\n",
    "\n",
    "    # Tokenize the inputs\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, max_length=max_input_length, padding=\"max_length\", truncation=True\n",
    "    )\n",
    "\n",
    "    # Tokenize the targets\n",
    "    labels = tokenizer(\n",
    "        queries, max_length=max_target_length, padding=\"max_length\", truncation=True\n",
    "    ).input_ids\n",
    "\n",
    "    # important: we need to replace the index of the padding tokens by -100\n",
    "    # such that they are not taken into account by the CrossEntropyLoss\n",
    "    labels_with_ignore_index = []\n",
    "    for labels_example in labels:\n",
    "        labels_example = [label if label != 0 else -100 for label in labels_example]\n",
    "        labels_with_ignore_index.append(labels_example)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels_with_ignore_index\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "# Map the function to each dataset\n",
    "train_dataset = train_dataset.map(preprocess_examples, batched=True)\n",
    "val_dataset = val_dataset.map(preprocess_examples, batched=True)\n",
    "test_dataset = test_dataset.map(preprocess_examples, batched=True)\n",
    "\n",
    "columns = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "\n",
    "# This sets `__getitem__` return format (type and columns). The data formatting is applied on-the-fly.\n",
    "# `__getitem__` is what pulls the batches during training\n",
    "train_dataset.set_format(type=\"torch\", columns=columns)\n",
    "val_dataset.set_format(type=\"torch\", columns=columns)\n",
    "test_dataset.set_format(type=\"torch\", columns=columns)\n",
    "\n",
    "print(\"Training\")\n",
    "print(train_dataset)\n",
    "print(\"*\" * 100)\n",
    "\n",
    "print(\"Validation\")\n",
    "print(val_dataset)\n",
    "print(\"*\" * 100)\n",
    "\n",
    "print(\"Test\")\n",
    "print(test_dataset)\n",
    "\n",
    "# Without the `.set_format`, this would get you all the columns\n",
    "print(train_dataset[0].keys())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that the previous is working as intended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataloader = DataLoader(val_dataset, batch_size=4)\n",
    "\n",
    "batch = next(iter(sample_dataloader))\n",
    "\n",
    "print(f\"The keys for each batch are:\")\n",
    "print(batch.keys())\n",
    "print(\"*\" * 100)\n",
    "\n",
    "print(\"Input token ids:\")\n",
    "print(batch[\"input_ids\"][0])\n",
    "print(\"*\" * 100)\n",
    "\n",
    "print(\"Decoded input tokens:\")\n",
    "print(tokenizer.decode(batch[\"input_ids\"][0]))\n",
    "print(\"*\" * 100)\n",
    "\n",
    "print(\"Label token ids:\")\n",
    "labels = batch[\"labels\"][0]\n",
    "print(labels)\n",
    "print(\"*\" * 100)\n",
    "\n",
    "print(\"Decoded label tokens:\")\n",
    "print(tokenizer.decode([label for label in labels if label != -100]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare and tune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "num_epochs = 5\n",
    "batch_size = 3\n",
    "test_batch_size = 2\n",
    "learning_rate = 5e-5\n",
    "# warmup_steps = 1000\n",
    "\n",
    "# Calculated values\n",
    "training_set_len = len(train_dataset)\n",
    "batches_per_epoch = math.ceil(training_set_len / batch_size)\n",
    "total_training_steps = num_epochs * batches_per_epoch\n",
    "warmup_steps = (\n",
    "    int(np.round(num_epochs / 3)) * batches_per_epoch\n",
    ")  # after a third of the epocs\n",
    "\n",
    "# Extra note-only parameters\n",
    "val_set_len = len(val_dataset)\n",
    "test_set_len = len(test_dataset)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=test_batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=test_batch_size)\n",
    "\n",
    "assert len(train_dataloader) == batches_per_epoch\n",
    "\n",
    "print(\"Training stats:\")\n",
    "print(f\"learning_rate: {learning_rate}\")\n",
    "pd.DataFrame(\n",
    "    index=[\n",
    "        \"TOTAL Samples\",\n",
    "        \"training_set_len\",\n",
    "        \"val_set_len\",\n",
    "        \"test_set_len\",\n",
    "        \"num_epochs\",\n",
    "        \"batch_size\",\n",
    "        \"batches_per_epoch\",\n",
    "        \"total_training_steps\",\n",
    "        \"warmup_steps\",\n",
    "    ],\n",
    "    columns=[\"training_value\"],\n",
    "    data=[\n",
    "        training_set_len + val_set_len + test_set_len,\n",
    "        training_set_len,\n",
    "        val_set_len,\n",
    "        test_set_len,\n",
    "        num_epochs,\n",
    "        batch_size,\n",
    "        batches_per_epoch,\n",
    "        total_training_steps,\n",
    "        warmup_steps,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpochProgressBar(TQDMProgressBar):\n",
    "    \"\"\"\n",
    "    This extends the base progress bar to not overwrite the progress bar\n",
    "    and show its history.\n",
    "    \"\"\"\n",
    "\n",
    "    def on_train_epoch_end(self, trainer=\"pl.Trainer\", pl_module=\"pl.LightningModule\"):\n",
    "        super().on_train_epoch_end(trainer=trainer, pl_module=pl_module)\n",
    "        print(\"\\n\")\n",
    "\n",
    "\n",
    "class HistoryCallback(Callback):\n",
    "    \"\"\"PyTorch Lightning metric callback.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.history = []\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        each_me = copy.deepcopy(trainer.callback_metrics)\n",
    "        self.history.append(each_me)\n",
    "\n",
    "    def history_dataframe(self):\n",
    "        return pd.DataFrame(self.history).astype(np.float32)\n",
    "\n",
    "\n",
    "class CodeT5(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_epochs,\n",
    "        batch_size,\n",
    "        lr,\n",
    "        training_set_len,\n",
    "        total_training_steps,\n",
    "        warmup_steps,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Model or layers\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(\n",
    "            \"Salesforce/codet5-small\"\n",
    "        )\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Debuggin vars(not needed for training)\n",
    "        self.training_steps_completed = 0.0\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids, attention_mask=attention_mask, labels=labels\n",
    "        )\n",
    "        return outputs\n",
    "\n",
    "    def common_step(self, batch, batch_idx):\n",
    "        # `batch` is a dictionary, the '**' before batch\n",
    "        # allows the 'forward step' to directly unpack the dictionary\n",
    "        outputs = self(**batch)\n",
    "\n",
    "        # The pretrained model aut calcs the loss\n",
    "        loss = outputs.loss\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self.common_step(batch, batch_idx)\n",
    "        self.log(\n",
    "            \"training_loss\",\n",
    "            loss,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "\n",
    "        last_lr = self.lr_schedulers().get_last_lr()[0]\n",
    "        self.log(\n",
    "            \"learning_rate\",\n",
    "            last_lr,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "\n",
    "        self.training_steps_completed += 1.0  # logger don't like ints\n",
    "        self.log(\n",
    "            \"training_steps_completed\",\n",
    "            self.training_steps_completed,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self.common_step(batch, batch_idx)\n",
    "        self.log(\n",
    "            \"validation_loss\",\n",
    "            loss,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss = self.common_step(batch, batch_idx)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # create optimizer\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "        # create learning rate scheduler\n",
    "        lr_scheduler = {\n",
    "            \"scheduler\": get_linear_schedule_with_warmup(\n",
    "                optimizer,\n",
    "                num_warmup_steps=self.hparams.warmup_steps,\n",
    "                num_training_steps=self.hparams.total_training_steps,\n",
    "            ),\n",
    "            \"name\": \"learning_rate\",\n",
    "            \"interval\": \"step\",\n",
    "            \"frequency\": 1,\n",
    "        }\n",
    "\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler}\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return train_dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return val_dataloader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return test_dataloader\n",
    "\n",
    "\n",
    "model = CodeT5(\n",
    "    num_epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    lr=learning_rate,\n",
    "    training_set_len=training_set_len,\n",
    "    total_training_steps=total_training_steps,\n",
    "    warmup_steps=warmup_steps,\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# Trainer section\n",
    "# -----------------------------------------------------------------------\n",
    "\n",
    "# Logger\n",
    "logger = CSVLogger(save_dir=LOGGER_FOLDER, name=\"My_Logger\")\n",
    "\n",
    "# Callbacks\n",
    "progress_bar = EpochProgressBar()\n",
    "history = HistoryCallback()\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    default_root_dir=CHECKPOINT_FOLDER,\n",
    "    callbacks=[progress_bar, history],\n",
    "    logger=logger,\n",
    "    max_epochs=num_epochs,\n",
    ")\n",
    "\n",
    "trainer.fit(model)\n",
    "\n",
    "history.history_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.save_pretrained(MODEL_FOLDER)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
