{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dmzfT-4OJnHA"
   },
   "outputs": [],
   "source": [
    "# %load_ext lab_black\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oT5BONOrJnHD"
   },
   "outputs": [],
   "source": [
    "# Set for local or colab\n",
    "\n",
    "import os\n",
    "from os.path import join\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "ASK_TO_DELETE_LOG_FOLDERS = False\n",
    "\n",
    "\n",
    "def check_create_folder(path: str, ask_to_rm_if_exists=ASK_TO_DELETE_LOG_FOLDERS):\n",
    "    if os.path.exists(path):\n",
    "        if ask_to_rm_if_exists:\n",
    "            response = input(\n",
    "                f\"<{path}>: Already exists.\\n\\nWrite 'del' if you wish to delete other wise press any key: \"\n",
    "            )\n",
    "            if response.lower() == \"del\":\n",
    "                print(f\"Deleting: {path}\")\n",
    "                shutil.rmtree(path)\n",
    "\n",
    "                os.makedirs(path)\n",
    "    else:\n",
    "        os.makedirs(path)\n",
    "\n",
    "\n",
    "# Check if running in colab\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "# Project defaults\n",
    "if IN_COLAB:\n",
    "    print(\"ENVIRONMENT: Colab\")\n",
    "\n",
    "    # Mount drive\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount(\"/content/drive\")\n",
    "\n",
    "    # Set the project directory\n",
    "    PROJECT_FOLDER = \"/content/drive/MyDrive/MIDS/w266/w266-project-carlos\"\n",
    "\n",
    "    # Install dependencies\n",
    "    !pip install -q transformers datasets pytorch-lightning SentencePiece #wandb\n",
    "else:\n",
    "    print(\"ENVIRONMENT: Local\")\n",
    "    # Set the project directory\n",
    "    PROJECT_FOLDER = \"/user/w266/w266-project-carlos\"\n",
    "\n",
    "os.chdir(PROJECT_FOLDER)\n",
    "\n",
    "# FOLDERS\n",
    "DATASET_FOLDER = join(PROJECT_FOLDER, \"dataset/dataset_final\")\n",
    "CHECKPOINT_FOLDER = join(PROJECT_FOLDER, \"checkpoints\")\n",
    "MODEL_FOLDER = join(PROJECT_FOLDER, \"saved_models\")\n",
    "LOGGER_FOLDER = join(PROJECT_FOLDER, \"logger\")\n",
    "\n",
    "check_create_folder(CHECKPOINT_FOLDER)\n",
    "check_create_folder(MODEL_FOLDER)\n",
    "check_create_folder(LOGGER_FOLDER)\n",
    "\n",
    "print(f\"Working directory is: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4E3jyLxOJnHG"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import (\n",
    "    Callback,\n",
    "    EarlyStopping,\n",
    "    LearningRateMonitor,\n",
    "    TQDMProgressBar,\n",
    ")\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    RobertaTokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0FwzqHhZbPmL"
   },
   "outputs": [],
   "source": [
    "from t5_model_support_functions import load_csv_files, token_to_df, expand_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fzjFb_QJnHH"
   },
   "source": [
    "### Load `csv` data as `dataframes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8js7ZC0gJnHI"
   },
   "outputs": [],
   "source": [
    "TARGET_FEATURES = [\"source\", \"labels\", \"token_types\"]\n",
    "\n",
    "df_train, df_val, df_test = load_csv_files(\n",
    "    [\n",
    "        join(DATASET_FOLDER, \"train.csv\"),\n",
    "        join(DATASET_FOLDER, \"dev.csv\"),\n",
    "        join(DATASET_FOLDER, \"test.csv\"),\n",
    "    ],\n",
    "    focus_columns=TARGET_FEATURES,\n",
    "    drop_duplicates=True,\n",
    "    dropna=True,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UF3VnhOgJnHJ"
   },
   "source": [
    "### Create the datasets (and set dataset length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0rUpcjnJJnHJ"
   },
   "outputs": [],
   "source": [
    "DEV_TESTING = False\n",
    "\n",
    "if DEV_TESTING:\n",
    "    train_dataset = Dataset.from_pandas(df_train.head(10), split=\"train\")\n",
    "    val_dataset = Dataset.from_pandas(df_val.head(10), split=\"validation\")\n",
    "    test_dataset = Dataset.from_pandas(df_test.head(10), split=\"test\")\n",
    "else:\n",
    "    train_dataset = Dataset.from_pandas(df_train, split=\"train\")\n",
    "    val_dataset = Dataset.from_pandas(df_val, split=\"validation\")\n",
    "    test_dataset = Dataset.from_pandas(df_test, split=\"test\")\n",
    "\n",
    "display(train_dataset)\n",
    "display(val_dataset)\n",
    "display(test_dataset)\n",
    "\n",
    "display(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sAR5chS7JnHK"
   },
   "source": [
    "### Pre-process and tokenize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-qJxDvqyJnHL"
   },
   "source": [
    "#### Select the pretrained model and tokenizer type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E-Zdj5x2JnHL"
   },
   "outputs": [],
   "source": [
    "# MODEL_TYPE = \"Salesforce/codet5-small\"\n",
    "# tokenizer_library = RobertaTokenizer\n",
    "\n",
    "# MODEL_TYPE = \"Salesforce/codet5-base\"\n",
    "# tokenizer_library = RobertaTokenizer\n",
    "\n",
    "MODEL_TYPE = \"Salesforce/codet5-large\"\n",
    "tokenizer_library = RobertaTokenizer\n",
    "\n",
    "# MODEL_TYPE = \"t5-small\"\n",
    "# tokenizer_library = T5Tokenizer\n",
    "\n",
    "# MODEL_TYPE = \"t5-base\"\n",
    "# tokenizer_library = T5Tokenizer\n",
    "\n",
    "# Get Tokenizer\n",
    "tokenizer = tokenizer_library.from_pretrained(MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eP6oTyL2JnHM"
   },
   "source": [
    "#### Declare and expand tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xnkzAccMJnHM"
   },
   "outputs": [],
   "source": [
    "new_tokens = [\n",
    "    \"<N>\",\n",
    "    \"</N>\",\n",
    "    \"<C>\",\n",
    "    \"[T]\",\n",
    "    \"[D]\",\n",
    "    \"[X]\",\n",
    "    \"[AggFunction]\",\n",
    "    \"[Y]\",\n",
    "    \"[Z]\",\n",
    "    \"[F]\",\n",
    "    \"[G]\",\n",
    "    \"[B]\",\n",
    "    \"[S]\",\n",
    "    \"[K]\",\n",
    "    \"</C>\",\n",
    "    \"<D>\",\n",
    "    \"<COL>\",\n",
    "    \"</COL>\",\n",
    "    \"<VAL>\",\n",
    "    \"</VAL>\",\n",
    "    \"</D>\",\n",
    "]\n",
    "\n",
    "# Expand Tokenzier with new Tokens\n",
    "display(expand_tokenizer(new_tokens, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hUd6yMyObPmR"
   },
   "source": [
    "#### Explore tokenized lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HuaJk_B2bPmR"
   },
   "outputs": [],
   "source": [
    "# Determine the maximum input length to not truncate any of the inputs\n",
    "# print(\"input token lengths\")\n",
    "# df_token_len = pd.DataFrame(\n",
    "#     columns=[\"token_count\"],\n",
    "#     data=[\n",
    "#         tokenizer(item, return_tensors=\"pt\")[\"input_ids\"].numpy().shape[1]\n",
    "#         for item in df_dataset[\"source\"]\n",
    "#     ],\n",
    "# )\n",
    "\n",
    "# display(df_token_len.describe(percentiles=[0.90, 0.95, 0.99, 0.999]))\n",
    "\n",
    "# display(df_token_len.query(\"token_count > 160\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L9r03wJXbPmS"
   },
   "outputs": [],
   "source": [
    "# Determine the maximum input length to not truncate any of the inputs\n",
    "# print(\"outputs token lengths\")\n",
    "# df_token_len = pd.DataFrame(\n",
    "#     columns=[\"token_count\"],\n",
    "#     data=[\n",
    "#         tokenizer(item, return_tensors=\"pt\")[\"input_ids\"].numpy().shape[1]\n",
    "#         for item in df_dataset[\"labels\"]\n",
    "#     ],\n",
    "# )\n",
    "\n",
    "# display(df_token_len.describe(percentiles=[0.90, 0.95, 0.99, 0.999]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cdoqm6kPbPmS"
   },
   "source": [
    "#### Set sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_XNbfl-XbPmT"
   },
   "outputs": [],
   "source": [
    "prefix = \"Generate vega_zero code: \"\n",
    "max_input_length = 162\n",
    "max_target_length = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4WpRVC4RJnHN"
   },
   "outputs": [],
   "source": [
    "def preprocess_examples(examples):\n",
    "    \"\"\"\n",
    "    This function process the input and targets (labels)\n",
    "\n",
    "    Inputs:\n",
    "    - Adds a prefix to the source (for t5)\n",
    "    - Tokenizes the input\n",
    "\n",
    "    Targets (labels):\n",
    "    - Tokenizes\n",
    "    - Replaces the padding token index from 0 to -100\n",
    "    \"\"\"\n",
    "    sources = examples[\"source\"]  # inputs\n",
    "    label_queries = examples[\"labels\"]  # targets\n",
    "\n",
    "    inputs = [prefix + source for source in sources]\n",
    "\n",
    "    # Tokenize the inputs\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=max_input_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # Tokenize the targets\n",
    "    labels = tokenizer(\n",
    "        label_queries,\n",
    "        max_length=max_target_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).input_ids\n",
    "\n",
    "    # important: we need to replace the index of the padding tokens by -100\n",
    "    # such that they are not taken into account by the CrossEntropyLoss\n",
    "    labels_with_ignore_index = []\n",
    "    for label_set in labels:\n",
    "        label_set = [label if label != 0 else -100 for label in label_set]\n",
    "        labels_with_ignore_index.append(label_set)\n",
    "\n",
    "    model_inputs[\"label_tokens\"] = labels_with_ignore_index\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "# Map the function to each dataset\n",
    "train_dataset = train_dataset.map(preprocess_examples, batched=True)\n",
    "val_dataset = val_dataset.map(preprocess_examples, batched=True)\n",
    "test_dataset = test_dataset.map(preprocess_examples, batched=True)\n",
    "\n",
    "columns = [\"input_ids\", \"attention_mask\", \"label_tokens\"]\n",
    "\n",
    "# This sets `__getitem__` return format (type and columns). The data formatting is applied on-the-fly.\n",
    "# `__getitem__` is what pulls the batches during training\n",
    "train_dataset.set_format(type=\"torch\", columns=columns)\n",
    "val_dataset.set_format(type=\"torch\", columns=columns)\n",
    "test_dataset.set_format(type=\"torch\", columns=columns)\n",
    "\n",
    "print(\"Training\")\n",
    "print(train_dataset)\n",
    "print(\"*\" * 100)\n",
    "\n",
    "print(\"Validation\")\n",
    "print(val_dataset)\n",
    "print(\"*\" * 100)\n",
    "\n",
    "print(\"Test\")\n",
    "print(test_dataset)\n",
    "\n",
    "# Without the `.set_format`, this would get you all the columns\n",
    "print(train_dataset[0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25IqRLDfJnHO"
   },
   "source": [
    "### Check that the previous is working as intended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0VA-igS7JnHO"
   },
   "outputs": [],
   "source": [
    "sample_dataloader = DataLoader(val_dataset, batch_size=4)\n",
    "\n",
    "batch = next(iter(sample_dataloader))\n",
    "\n",
    "print(f\"The keys for each batch are:\")\n",
    "print(batch.keys())\n",
    "print(\"*\" * 100)\n",
    "\n",
    "print(\"Input token ids:\")\n",
    "print(batch[\"input_ids\"][0])\n",
    "print(\"*\" * 100)\n",
    "\n",
    "print(\"Decoded input tokens:\")\n",
    "print(tokenizer.decode(batch[\"input_ids\"][0]))\n",
    "print(\"*\" * 100)\n",
    "\n",
    "print(\"Label token ids:\")\n",
    "labels = batch[\"label_tokens\"][0]\n",
    "print(labels)\n",
    "print(\"*\" * 100)\n",
    "\n",
    "print(\"Decoded label tokens:\")\n",
    "print(tokenizer.decode([label for label in labels if label != -100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NSaM-F-8JnHO"
   },
   "source": [
    "### Set hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e4Lyr-KNJnHP"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device type: {device}\")\n",
    "\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# hyper parameters\n",
    "num_epochs = 5\n",
    "batch_size = 15\n",
    "test_batch_size = 3\n",
    "learning_rate = 5e-5\n",
    "# warmup_steps = 1000\n",
    "\n",
    "# Calculated values\n",
    "training_set_len = len(train_dataset)\n",
    "batches_per_epoch = math.ceil(training_set_len / batch_size)\n",
    "total_training_steps = num_epochs * batches_per_epoch\n",
    "warmup_steps = (\n",
    "    int(np.round(num_epochs / 3)) * batches_per_epoch\n",
    ")  # after a third of the epocs\n",
    "\n",
    "# Extra note-only parameters\n",
    "val_set_len = len(val_dataset)\n",
    "test_set_len = len(test_dataset)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=test_batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=test_batch_size)\n",
    "\n",
    "assert len(train_dataloader) == batches_per_epoch\n",
    "\n",
    "print(\"Training stats:\")\n",
    "print(f\"learning_rate: {learning_rate}\")\n",
    "pd.DataFrame(\n",
    "    index=[\n",
    "        \"TOTAL Samples\",\n",
    "        \"training_set_len\",\n",
    "        \"val_set_len\",\n",
    "        \"test_set_len\",\n",
    "        \"num_epochs\",\n",
    "        \"batch_size\",\n",
    "        \"batches_per_epoch\",\n",
    "        \"total_training_steps\",\n",
    "        \"warmup_steps\",\n",
    "    ],\n",
    "    columns=[\"training_value\"],\n",
    "    data=[\n",
    "        training_set_len + val_set_len + test_set_len,\n",
    "        training_set_len,\n",
    "        val_set_len,\n",
    "        test_set_len,\n",
    "        num_epochs,\n",
    "        batch_size,\n",
    "        batches_per_epoch,\n",
    "        total_training_steps,\n",
    "        warmup_steps,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_DAMQiXAbPmV"
   },
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IfXmU40OJnHP"
   },
   "outputs": [],
   "source": [
    "class EpochProgressBar(TQDMProgressBar):\n",
    "    \"\"\"\n",
    "    This extends the base progress bar to not overwrite the progress bar\n",
    "    and show its history.\n",
    "    \"\"\"\n",
    "\n",
    "    def on_train_epoch_end(self, trainer=\"pl.Trainer\", pl_module=\"pl.LightningModule\"):\n",
    "        super().on_train_epoch_end(trainer=trainer, pl_module=pl_module)\n",
    "        print(\"\\n\")\n",
    "\n",
    "\n",
    "class HistoryCallback(Callback):\n",
    "    \"\"\"PyTorch Lightning metric callback.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.history = []\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        each_me = copy.deepcopy(trainer.callback_metrics)\n",
    "        self.history.append(each_me)\n",
    "\n",
    "    def history_dataframe(self):\n",
    "        return pd.DataFrame(self.history).astype(np.float32)\n",
    "\n",
    "\n",
    "class CodeT5(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_type,\n",
    "        num_epochs,\n",
    "        batch_size,\n",
    "        lr,\n",
    "        training_set_len,\n",
    "        total_training_steps,\n",
    "        warmup_steps,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Model or layers\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(model_type)\n",
    "\n",
    "        self.model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Debuggin vars(not needed for training)\n",
    "        self.training_steps_completed = 0.0\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, label_tokens=None):\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids, attention_mask=attention_mask, labels=label_tokens\n",
    "        )\n",
    "        return outputs\n",
    "\n",
    "    def common_step(self, batch, batch_idx):\n",
    "        # `batch` is a dictionary, the '**' before batch\n",
    "        # allows the 'forward step' to directly unpack the dictionary\n",
    "        outputs = self(**batch)\n",
    "\n",
    "        # The pretrained model aut calcs the loss\n",
    "        loss = outputs.loss\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self.common_step(batch, batch_idx)\n",
    "        self.log(\n",
    "            \"training_loss\",\n",
    "            loss,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "\n",
    "        last_lr = self.lr_schedulers().get_last_lr()[0]\n",
    "        self.log(\n",
    "            \"learning_rate\",\n",
    "            last_lr,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "\n",
    "        self.training_steps_completed += 1.0  # logger don't like ints\n",
    "        self.log(\n",
    "            \"training_steps_completed\",\n",
    "            self.training_steps_completed + 0.5,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self.common_step(batch, batch_idx)\n",
    "        self.log(\n",
    "            \"validation_loss\",\n",
    "            loss,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss = self.common_step(batch, batch_idx)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # create optimizer\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "        # create learning rate scheduler\n",
    "        lr_scheduler = {\n",
    "            \"scheduler\": get_linear_schedule_with_warmup(\n",
    "                optimizer,\n",
    "                num_warmup_steps=self.hparams.warmup_steps,\n",
    "                num_training_steps=self.hparams.total_training_steps,\n",
    "            ),\n",
    "            \"name\": \"learning_rate\",\n",
    "            \"interval\": \"step\",\n",
    "            \"frequency\": 1,\n",
    "        }\n",
    "\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler}\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return train_dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return val_dataloader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return test_dataloader\n",
    "\n",
    "\n",
    "model = CodeT5(\n",
    "    model_type=MODEL_TYPE,\n",
    "    num_epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    lr=learning_rate,\n",
    "    training_set_len=training_set_len,\n",
    "    total_training_steps=total_training_steps,\n",
    "    warmup_steps=warmup_steps,\n",
    ")\n",
    "\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# Trainer section\n",
    "# -----------------------------------------------------------------------\n",
    "\n",
    "# Logger\n",
    "logger = CSVLogger(save_dir=LOGGER_FOLDER, name=\"CodeT5_Logger\")\n",
    "\n",
    "# Callbacks\n",
    "progress_bar = EpochProgressBar()\n",
    "history = HistoryCallback()\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    default_root_dir=CHECKPOINT_FOLDER,\n",
    "    callbacks=[progress_bar, history],\n",
    "    logger=logger,\n",
    "    max_epochs=num_epochs,\n",
    ")\n",
    "\n",
    "trainer.fit(model)\n",
    "\n",
    "history.history_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fz7AHjgRJnHQ"
   },
   "outputs": [],
   "source": [
    "model.model.save_pretrained(MODEL_FOLDER)\n",
    "tokenizer.save_pretrained(join(MODEL_FOLDER, \"tokenizer\"))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
